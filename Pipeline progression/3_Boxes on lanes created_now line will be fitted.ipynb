{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The goals / steps of this project are the following:\n",
    "\n",
    "1) Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n",
    "\n",
    "2) Apply a distortion correction to raw images.\n",
    "\n",
    "3) Use color transforms, gradients, etc., to create a thresholded binary image.\n",
    "\n",
    "4) Apply a perspective transform to rectify binary image (\"birds-eye view\").\n",
    "\n",
    "5) Detect lane pixels and fit to find the lane boundary.\n",
    "\n",
    "6) Determine the curvature of the lane and vehicle position with respect to center.\n",
    "\n",
    "7) Warp the detected lane boundaries back onto the original image.\n",
    "\n",
    "8) Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strategy: \n",
    "    - Generate and save transformation matrices to undistort images\n",
    "    - Create undistort function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n",
    "\n",
    "The images for camera calibration are stored in the folder called camera_cal. The images in test_images are for testing your pipeline on single frames. If you want to extract more test images from the videos, you can simply use an image writing method like cv2.imwrite(), i.e., you can read the video in frame by frame as usual, and for frames you want to save for later you can write to an image file.\n",
    "\n",
    "## Saves the relevant transformation matrices in a pickle file for saving time in the next set of codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib notebook\n",
    "\n",
    "\"\"\"A: Finding image and object points\"\"\"\n",
    "\n",
    "def undistort(test_img):\n",
    "# prepare object points (our ideal reference), like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "# Stores mtx and dist coefficients in a pickle file to use later\n",
    "    nx=9    # Number of inner corners of our chessboard along x axis (or columns)\n",
    "    ny=6    # Number of inner corners of our chessboard along y axis (or rows)\n",
    "\n",
    "    objp = np.zeros((ny*nx,3), np.float32)                  #We have 9 corners on X axis and 6 corners on Y axis\n",
    "    objp[:,:2] = np.mgrid[0:nx, 0:ny].T.reshape(-1,2)       # Gives us coorinate points in pairs as a list of 54 items. It's shape will be (54,2)       \n",
    "\n",
    "    # Arrays to store object points and image points from all the images.\n",
    "    objpoints = [] # 3d points in real world space. These are the points for our ideal chessboard which we are using as a reference.       \n",
    "    imgpoints = [] # 2d points in image plane. We'll extract these from the images given for caliberating the camera\n",
    "\n",
    "    # Make a list of calibration images\n",
    "    images = glob.glob('camera_cal/calibration*.jpg')\n",
    "\n",
    "    # Step through the list and search for chessboard corners\n",
    "    for idx, fname in enumerate(images):\n",
    "        calib_img = cv2.imread(fname)\n",
    "        gray = cv2.cvtColor(calib_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Find the chessboard corners\n",
    "        # Grayscale conversion ensures an 8bit image as input.The next function needs that kind of input only. Generally color images are 24 bit images. (Refer \"Bits in images\" in notes) \n",
    "        ret, corners = cv2.findChessboardCorners(gray, (nx,ny), None)\n",
    "\n",
    "        # If found, add object points, image points\n",
    "        if ret == True:\n",
    "            objpoints.append(objp)      # These will be same for caliberation image. The same points will get appended every time this fires up \n",
    "            imgpoints.append(corners)   # Corners \n",
    "            \n",
    "            # Draw and display the corners                                  #This step can be completely skipped\n",
    "            cv2.drawChessboardCorners(calib_img, (nx,ny), corners, ret)\n",
    "            write_name = 'corners_found'+str(idx)+'.jpg'\n",
    "            cv2.imwrite('output_files/corners_found_for_calib/'+write_name, calib_img)  \n",
    "            cv2.imshow(write_name, calib_img)  #We dont want to see the images now so commenting out. TO see output later, un-comment these 3 lines\n",
    "            cv2.waitKey(500)   #Delete after testing. These will be used to show you images one after the other\n",
    "\n",
    "    cv2.destroyAllWindows()   #Delete this after testing\n",
    "    \n",
    "    # Test undistortion on an image\n",
    "\n",
    "    test_img_size = (test_img.shape[1], test_img.shape[0])\n",
    "    \n",
    "    # Do camera calibration given object points and image points\n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, test_img_size,None,None)\n",
    "    \n",
    "    # Use the above obtained results to undistort \n",
    "    undist_img = cv2.undistort(test_img, mtx, dist, None, mtx)\n",
    "    \n",
    "    cv2.imwrite('output_files/test_undist.jpg',undist_img)\n",
    "    \n",
    "    # Save the camera calibration result for later use (we won't worry about rvecs / tvecs)\n",
    "    dist_pickle = {}\n",
    "    dist_pickle[\"mtx\"] = mtx\n",
    "    dist_pickle[\"dist\"] = dist\n",
    "    pickle.dump( dist_pickle, open( \"output_files/calib_pickle_files/dist_pickle.p\", \"wb\" ) )\n",
    "    #undist_img = cv2.cvtColor(dst, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    return undist_img\n",
    "    \n",
    "    \n",
    "\n",
    "test_img= cv2.imread('camera_cal/calibration1.jpg')    #Note: Your image will be in BGR format\n",
    "output=undistort(test_img)\n",
    "\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))        #Refer subplots in python libraries\n",
    "ax1.imshow(test_img)\n",
    "ax1.set_title('Original Image', fontsize=30)\n",
    "ax2.imshow(output)\n",
    "ax2.set_title('Undistorted Image', fontsize=30)\n",
    "cv2.waitKey(500)\n",
    "cv2.destroyAllWindows()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Apply a distortion correction to raw images\n",
    "\n",
    "Now we'll use the transformation matrices stored in the pickle file above and try undistorting example images\n",
    "\n",
    "Precaution: If you're reading colored image with cv2, convert it to RGB from BGR before using ax.imshow(). \n",
    "\n",
    "Reason: It requred a RGB image if it is 3D\n",
    "\n",
    "So I'm leaving a comment in my *\"cal_undistort function\"* to do the conversion in case you use cv2 to read frames and plan to output using ax.imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "\n",
    "def cal_undistort(img):\n",
    "# Reads mtx and dist matrices, peforms image distortion correction and returns the undistorted image\n",
    "\n",
    "    import pickle\n",
    "        \n",
    "    # Read in the saved matrices\n",
    "    my_dist_pickle = pickle.load( open( \"output_files/calib_pickle_files/dist_pickle.p\", \"rb\" ) )\n",
    "    mtx = my_dist_pickle[\"mtx\"]\n",
    "    dist = my_dist_pickle[\"dist\"]\n",
    "\n",
    "    img_size = (img.shape[1], img.shape[0])    \n",
    "\n",
    "    undistorted_img = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    #undistorted_img =  cv2.cvtColor(undistorted_img, cv2.COLOR_BGR2RGB)   #Use if you use cv2 to import image. ax.imshow() needs RGB image\n",
    "    return undistorted_img\n",
    "\n",
    "def draw_subplot(img1,name1,img2,name2):\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "    f.tight_layout()\n",
    "    ax1.imshow(img1)  #Needs an RGB image for 3D images. For 2D images, it auto-colors them so use cmap='gray' to get grayscale if needed\n",
    "    ax1.set_title(name1, fontsize=50)\n",
    "    ax2.imshow(img2)\n",
    "    ax2.set_title(name2, fontsize=50)\n",
    "    plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\n",
    "\n",
    "# Read in an image\n",
    "img = mpimg.imread('test_images/test2.jpg')           # highway image\n",
    "#img = mpimg.imread('camera_cal/calibration3.jpg')    # chessboard image\n",
    "\n",
    "undistorted = cal_undistort(img)\n",
    "\n",
    "draw_subplot(img,\"OG image\",undistorted,\"Undist image\")\n",
    "\n",
    "print(\"To note the changes, look carefully at the outer boundary of both the images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Use color transforms, gradients, etc., to create a thresholded binary image.\n",
    "\n",
    "\n",
    "Caution: In the thresh_img() function, we begin by coverting our color space from RGB to HLS. We need to check whether our image was RGB or BGR when it was extracted from the frame?\n",
    "\n",
    "Note: Put undistorted RGB images in this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def thresh_img(img):\n",
    "    \"\"\"\n",
    "    x gradient will identify lanes far away from us \n",
    "    Saturation channel will help us with the lanes near us. This will help if there's a lot of light\n",
    "    \"\"\"    \n",
    "    \n",
    "    \"\"\"Starting with color channel\"\"\"\n",
    "    # Convert to HLS color space and separate the S channel\n",
    "    # Note: img is the undistorted image\n",
    "    hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "    s_channel = hls[:,:,2]\n",
    "    h_channel = hls[:,:,0]\n",
    "    # Threshold color channel\n",
    "    s_thresh_min = 170\n",
    "    s_thresh_max = 255\n",
    "    \n",
    "    h_thresh_min = 21\n",
    "    h_thresh_max = 22\n",
    "        \n",
    "    s_binary = np.zeros_like(s_channel)\n",
    "    s_binary[(s_channel >= s_thresh_min) & (s_channel <= s_thresh_max)] = 1\n",
    "\n",
    "    h_binary = np.zeros_like(h_channel)\n",
    "    h_binary[(h_channel >= h_thresh_min) & (h_channel <= h_thresh_max)] = 1\n",
    "\n",
    "    \n",
    "    \"\"\"Now handling the x gradient\"\"\"\n",
    "    # Grayscale image\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    # Sobel x\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0) # Take the derivative in x\n",
    "    abs_sobelx = np.absolute(sobelx) # Absolute x derivative to accentuate lines away from horizontal\n",
    "    scaled_sobel = np.uint8(255*abs_sobelx/np.max(abs_sobelx))\n",
    "    # Threshold x gradient\n",
    "    thresh_min = 20\n",
    "    thresh_max = 100\n",
    "    sxbinary = np.zeros_like(scaled_sobel)\n",
    "    sxbinary[(scaled_sobel >= thresh_min) & (scaled_sobel <= thresh_max)] = 1\n",
    "        \n",
    "    # Combine the two binary thresholds\n",
    "    combined_binary = np.zeros_like(sxbinary)\n",
    "    combined_binary[((s_binary == 1) & (h_binary == 1)) | (sxbinary == 1)] = 1\n",
    "    #Used h as well so as to reduce noise in the image\n",
    "    \n",
    "    out_img = np.dstack((combined_binary, combined_binary, combined_binary))*255\n",
    "    \n",
    "    #return combined_binary\n",
    "    return out_img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Apply a perspective transform to rectify binary image (\"birds-eye view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perspective_transform(img):\n",
    "    \n",
    "    # Define calibration box in source (original) and destination (desired or warped) coordinates\n",
    "    \n",
    "    img_size = (img.shape[1], img.shape[0])\n",
    "    \"\"\"Notice the format used for img_size. Yaha bhi ulta hai. x axis aur fir y axis chahiye. \n",
    "          Apne format mein rows(y axis) and columns (x axis) hain\"\"\"\n",
    "    \n",
    "    \n",
    "    # Four source coordinates\n",
    "\n",
    "    src = np.array(\n",
    "        [[437*img.shape[1]/960, 331*img.shape[0]/540],\n",
    "         [523*img.shape[1]/960, 331*img.shape[0]/540],\n",
    "         [850*img.shape[1]/960, img.shape[0]],\n",
    "         [145*img.shape[1]/960, img.shape[0]]], dtype='f')\n",
    "    \n",
    "    \n",
    "    # Next, we'll define a desired rectangle plane for the warped image.\n",
    "    # We'll choose 4 points where we want source points to end up \n",
    "    # This time we'll choose our points by eyeballing a rectangle\n",
    "    \n",
    "    dst = np.array(\n",
    "        [[290*img.shape[1]/960, 0],\n",
    "         [740*img.shape[1]/960, 0],\n",
    "         [740*img.shape[1]/960, img.shape[0]],\n",
    "         [290*img.shape[1]/960, img.shape[0]]], dtype='f')\n",
    "    \n",
    "    \n",
    "    #Compute the perspective transform, M, given source and destination points:\n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "  \n",
    "    #Warp an image using the perspective transform, M; using linear interpolation    \n",
    "    #Interpolating points is just filling in missing points as it warps an image\n",
    "    # The input image for this function can be a colored image too\n",
    "    warped = cv2.warpPerspective(img, M, img_size, flags=cv2.INTER_LINEAR)\n",
    "    \n",
    "    return warped,src,dst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "def process_image(frame):\n",
    "    \n",
    "    def cal_undistort(img):\n",
    "        # Reads mtx and dist matrices, peforms image distortion correction and returns the undistorted image\n",
    "\n",
    "        import pickle\n",
    "\n",
    "        # Read in the saved matrices\n",
    "        my_dist_pickle = pickle.load( open( \"output_files/calib_pickle_files/dist_pickle.p\", \"rb\" ) )\n",
    "        mtx = my_dist_pickle[\"mtx\"]\n",
    "        dist = my_dist_pickle[\"dist\"]\n",
    "\n",
    "        img_size = (img.shape[1], img.shape[0])    \n",
    "\n",
    "        undistorted_img = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "        #undistorted_img =  cv2.cvtColor(undistorted_img, cv2.COLOR_BGR2RGB)   #Use if you use cv2 to import image. ax.imshow() needs RGB image\n",
    "        return undistorted_img\n",
    "\n",
    "    \n",
    "    def yellow_threshold(img, sxbinary):\n",
    "        # Convert to HLS color space and separate the S channel\n",
    "        # Note: img is the undistorted image\n",
    "        hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "        s_channel = hls[:,:,2]\n",
    "        h_channel = hls[:,:,0]\n",
    "        # Threshold color channel\n",
    "        s_thresh_min = 100\n",
    "        s_thresh_max = 255\n",
    "        \n",
    "        #for 360 degree, my value for yellow ranged between 35 and 50. So uska half kar diya\n",
    "        h_thresh_min = 10    \n",
    "        h_thresh_max = 25\n",
    "\n",
    "        s_binary = np.zeros_like(s_channel)\n",
    "        s_binary[(s_channel >= s_thresh_min) & (s_channel <= s_thresh_max)] = 1\n",
    "\n",
    "        h_binary = np.zeros_like(h_channel)\n",
    "        h_binary[(h_channel >= h_thresh_min) & (h_channel <= h_thresh_max)] = 1\n",
    "\n",
    "        # Combine the two binary thresholds\n",
    "        yellow_binary = np.zeros_like(s_binary)\n",
    "        yellow_binary[(((s_binary == 1) | (sxbinary == 1) ) & (h_binary ==1))] = 1\n",
    "        return yellow_binary\n",
    "    \n",
    "    def xgrad_binary(img, thresh_min=30, thresh_max=100):\n",
    "        # Grayscale image\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        # Sobel x\n",
    "        sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0) # Take the derivative in x\n",
    "        abs_sobelx = np.absolute(sobelx) # Absolute x derivative to accentuate lines away from horizontal\n",
    "        scaled_sobel = np.uint8(255*abs_sobelx/np.max(abs_sobelx))\n",
    "        # Threshold x gradient\n",
    "        #thresh_min = 30    #Already given above\n",
    "        #thresh_max = 100\n",
    "\n",
    "        sxbinary = np.zeros_like(scaled_sobel)\n",
    "        sxbinary[(scaled_sobel >= thresh_min) & (scaled_sobel <= thresh_max)] = 1\n",
    "        return sxbinary\n",
    "    \n",
    "    def white_threshold(img, sxbinary, lower_white_thresh = 170):\n",
    "        r_channel = img[:,:,0]\n",
    "        g_channel = img[:,:,1]\n",
    "        b_channel = img[:,:,2]\n",
    "        # Threshold color channel\n",
    "        r_thresh_min = lower_white_thresh\n",
    "        r_thresh_max = 255\n",
    "        r_binary = np.zeros_like(r_channel)\n",
    "        r_binary[(r_channel >= r_thresh_min) & (r_channel <= r_thresh_max)] = 1\n",
    "        \n",
    "        g_thresh_min = lower_white_thresh\n",
    "        g_thresh_max = 255\n",
    "        g_binary = np.zeros_like(g_channel)\n",
    "        g_binary[(g_channel >= g_thresh_min) & (g_channel <= g_thresh_max)] = 1\n",
    "\n",
    "        b_thresh_min = lower_white_thresh\n",
    "        b_thresh_max = 255\n",
    "        b_binary = np.zeros_like(b_channel)\n",
    "        b_binary[(b_channel >= b_thresh_min) & (b_channel <= b_thresh_max)] = 1\n",
    "\n",
    "        white_binary = np.zeros_like(r_channel)\n",
    "        white_binary[((r_binary ==1) & (g_binary ==1) & (b_binary ==1) & (sxbinary==1))] = 1\n",
    "        return white_binary\n",
    "        \n",
    "    def thresh_img(img):\n",
    "                       \n",
    "       \n",
    "        #sxbinary = xgrad_binary(img, thresh_min=30, thresh_max=100)\n",
    "        sxbinary = xgrad_binary(img, thresh_min=25, thresh_max=130)\n",
    "        yellow_binary = yellow_threshold(img, sxbinary)     #(((s) | (sx)) & (h))\n",
    "        white_binary = white_threshold(img, sxbinary, lower_white_thresh = 150)\n",
    "        \n",
    "        # Combine the two binary thresholds\n",
    "        combined_binary = np.zeros_like(sxbinary)\n",
    "        combined_binary[((yellow_binary == 1) | (white_binary == 1))] = 1\n",
    "        \n",
    "        out_img = np.dstack((combined_binary, combined_binary, combined_binary))*255\n",
    "        \n",
    "        return out_img\n",
    "    \n",
    "    def perspective_transform(img):\n",
    "    \n",
    "        # Define calibration box in source (original) and destination (desired or warped) coordinates\n",
    "\n",
    "        img_size = (img.shape[1], img.shape[0])\n",
    "        \"\"\"Notice the format used for img_size. Yaha bhi ulta hai. x axis aur fir y axis chahiye. \n",
    "              Apne format mein rows(y axis) and columns (x axis) hain\"\"\"\n",
    "\n",
    "\n",
    "        # Four source coordinates\n",
    "        # Order of points: top left, top right, bottom right, bottom left\n",
    "        \n",
    "        src = np.array(\n",
    "            [[435*img.shape[1]/960, 350*img.shape[0]/540],\n",
    "             [535*img.shape[1]/960, 350*img.shape[0]/540],\n",
    "             [885*img.shape[1]/960, img.shape[0]],\n",
    "             [220*img.shape[1]/960, img.shape[0]]], dtype='f')\n",
    "        \n",
    "\n",
    "        # Next, we'll define a desired rectangle plane for the warped image.\n",
    "        # We'll choose 4 points where we want source points to end up \n",
    "        # This time we'll choose our points by eyeballing a rectangle\n",
    "\n",
    "        dst = np.array(\n",
    "            [[290*img.shape[1]/960, 0],\n",
    "             [740*img.shape[1]/960, 0],\n",
    "             [740*img.shape[1]/960, img.shape[0]],\n",
    "             [290*img.shape[1]/960, img.shape[0]]], dtype='f')\n",
    "\n",
    "\n",
    "        #Compute the perspective transform, M, given source and destination points:\n",
    "        M = cv2.getPerspectiveTransform(src, dst)\n",
    "\n",
    "        #Warp an image using the perspective transform, M; using linear interpolation    \n",
    "        #Interpolating points is just filling in missing points as it warps an image\n",
    "        # The input image for this function can be a colored image too\n",
    "        warped = cv2.warpPerspective(img, M, img_size, flags=cv2.INTER_LINEAR)\n",
    "              \n",
    "        return warped, src, dst \n",
    "    \n",
    "    def draw_polygon(img1, img2, src, dst):\n",
    "        src = src.astype(int)  #Very important step (Pixels cannot be in decimals)\n",
    "        dst = dst.astype(int)\n",
    "        cv2.polylines(img1, [src], True, (255,0,0), 3)\n",
    "        cv2.polylines(img2, [dst], True, (255,0,0), 3)\n",
    "    \n",
    "    def histogram_bottom_peaks (warped_img):\n",
    "        # This will detect the bottom point of our lane lines\n",
    "        \n",
    "        # Take a histogram of the bottom half of the image\n",
    "        bottom_half = warped_img[(warped_img.shape[0]//2):,:,0]     # Collecting all pixels in the bottom half\n",
    "        histogram = np.sum(bottom_half, axis=0)                         # Summing them along y axis (or along columns)\n",
    "        # Find the peak of the left and right halves of the histogram\n",
    "        # These will be the starting point for the left and right lines\n",
    "        midpoint = np.int(histogram.shape[0]//2)        # 1D array hai histogram toh uska bas 0th index filled hoga \n",
    "        #print(np.shape(histogram))     #OUTPUT:(1280,)\n",
    "        leftx_base = np.argmax(histogram[:midpoint])\n",
    "        rightx_base = np.argmax(histogram[midpoint:]) + midpoint\n",
    "\n",
    "        return leftx_base, rightx_base\n",
    "    \n",
    "    def find_lane_pixels(warped_img):\n",
    "    \n",
    "        leftx_base, rightx_base = histogram_bottom_peaks(warped_img)\n",
    "   \n",
    "        # Create an output image to draw on and visualize the result\n",
    "        out_img = np.copy(warped_img)\n",
    "        \n",
    "        # HYPERPARAMETERS\n",
    "        # Choose the number of sliding windows\n",
    "        nwindows = 9\n",
    "        # Set the width of the windows +/- margin. So width = 2*margin \n",
    "        margin = 100\n",
    "        # Set minimum number of pixels found to recenter window\n",
    "        minpix = 50\n",
    "    \n",
    "        # Set height of windows - based on nwindows above and image shape\n",
    "        window_height = np.int(warped_img.shape[0]//nwindows)\n",
    "        # Identify the x and y positions of all nonzero pixels in the image\n",
    "        nonzero = warped_img.nonzero()  #pixel ke coordinates dega 2 seperate arrays mein\n",
    "        nonzeroy = np.array(nonzero[0])    # Y coordinates milenge 1D array mein. They will we arranged in the order of pixels\n",
    "        nonzerox = np.array(nonzero[1])\n",
    "        # Current positions to be updated later for each window in nwindows\n",
    "        leftx_current = leftx_base         #initially set kar diya hai. For loop ke end mein change karenge\n",
    "        rightx_current = rightx_base\n",
    "\n",
    "        # Create empty lists to receive left and right lane pixel indices\n",
    "        left_lane_inds = []   # Ismein lane-pixels ke indices collect karenge. \n",
    "                              # 'nonzerox' array mein index daalke coordinate mil jaayega\n",
    "        right_lane_inds = []  \n",
    "\n",
    "        # Step through the windows one by one\n",
    "        for window in range(nwindows):\n",
    "            # Identify window boundaries in x and y (and right and left)\n",
    "            win_y_low = warped_img.shape[0] - (window+1)*window_height\n",
    "            win_y_high = warped_img.shape[0] - window*window_height\n",
    "            \"\"\"### TO-DO: Find the four below boundaries of the window ###\"\"\"\n",
    "            win_xleft_low = leftx_current - margin  \n",
    "            win_xleft_high = leftx_current + margin  \n",
    "            win_xright_low = rightx_current - margin  \n",
    "            win_xright_high = rightx_current + margin \n",
    "\n",
    "            # Draw the windows on the visualization image\n",
    "            cv2.rectangle(out_img,(win_xleft_low,win_y_low),\n",
    "            (win_xleft_high,win_y_high),(0,255,0), 2) \n",
    "            cv2.rectangle(out_img,(win_xright_low,win_y_low),\n",
    "            (win_xright_high,win_y_high),(0,255,0), 2) \n",
    "            \n",
    "\n",
    "            ### TO-DO: Identify the nonzero pixels in x and y within the window ###\n",
    "            #Iska poora explanation seperate page mein likha hai\n",
    "            good_left_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & \n",
    "            (nonzerox >= win_xleft_low) &  (nonzerox < win_xleft_high)).nonzero()[0]\n",
    "\n",
    "            good_right_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & \n",
    "            (nonzerox >= win_xright_low) &  (nonzerox < win_xright_high)).nonzero()[0]\n",
    "\n",
    "\n",
    "            # Append these indices to the lists\n",
    "            left_lane_inds.append(good_left_inds)\n",
    "            right_lane_inds.append(good_right_inds)\n",
    "\n",
    "            # If you found > minpix pixels, recenter next window on the mean position of the pixels in your current window (re-centre)\n",
    "            if len(good_left_inds) > minpix:\n",
    "                leftx_current = np.int(np.mean(nonzerox[good_left_inds]))\n",
    "            if len(good_right_inds) > minpix:        \n",
    "                rightx_current = np.int(np.mean(nonzerox[good_right_inds]))\n",
    "    \n",
    "    \n",
    "        # Concatenate the arrays of indices (previously was a list of lists of pixels)\n",
    "        try:\n",
    "            left_lane_inds = np.concatenate(left_lane_inds)\n",
    "            right_lane_inds = np.concatenate(right_lane_inds)\n",
    "        except ValueError:\n",
    "            # Avoids an error if the above is not implemented fully\n",
    "            pass\n",
    "    \n",
    "        # Extract left and right line pixel positions\n",
    "        leftx = nonzerox[left_lane_inds]\n",
    "        lefty = nonzeroy[left_lane_inds] \n",
    "        rightx = nonzerox[right_lane_inds]\n",
    "        righty = nonzeroy[right_lane_inds]\n",
    "\n",
    "        #return leftx, lefty, rightx, righty, out_img\n",
    "        \n",
    "        \"\"\"\"\"\"\n",
    "        print(leftx)\n",
    "        print(lefty)\n",
    "        print(\"\")\n",
    "        print(rightx)\n",
    "        print(righty)\n",
    "        \n",
    "        \n",
    "        \n",
    "        left_fit = np.polyfit(lefty,leftx,2)\n",
    "        right_fit = np.polyfit(righty,rightx,2)\n",
    "        \n",
    "        \n",
    "        \"\"\"\"\"\"\n",
    "        return out_img\n",
    "        \n",
    "\n",
    "    \n",
    "    undist_img = cal_undistort(frame)\n",
    "    thresh_img = thresh_img(undist_img)    # Note: This is not a binary iamge. It has been stacked already within the function\n",
    "    warped_img, src, dst = perspective_transform(thresh_img)\n",
    "    draw_polygon(frame, warped_img, src, dst)   #the first image is the original image that you import into the system\n",
    "    lane_identified = find_lane_pixels(warped_img)\n",
    "    \n",
    "    #return thresh_img, warped_img    #3 images dekhne ke liye ye return\n",
    "    return warped_img, lane_identified          #video chalane ke liye ye return\n",
    "    #return lane_identified\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Detect lane pixels and fit to find the lane boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image = mpimg.imread(\"my_test_images/starter.JPG\")\n",
    "#image = mpimg.imread(\"my_test_images/straight_road.JPG\")                         #top left corner thoda right\n",
    "image = mpimg.imread(\"my_test_images/change_road_color.JPG\")        #too less data points in right lane\n",
    "#image = mpimg.imread(\"my_test_images/leaving_tree_to_road_color_change.JPG\")\n",
    "#image = mpimg.imread(\"my_test_images/tree_and_color_change.JPG\")\n",
    "#image = mpimg.imread(\"my_test_images/trees_left_lane_missing.JPG\")\n",
    "#image = mpimg.imread(\"my_test_images/trees_left_lane_missing2.JPG\")\n",
    "#image = mpimg.imread(\"my_test_images/1.JPG\")\n",
    "#image = mpimg.imread(\"my_test_images/2.JPG\")                     #too less data points in right lane\n",
    "#image = mpimg.imread(\"my_test_images/3.JPG\")                 #too less points in right lane\n",
    "#image = mpimg.imread(\"my_test_images/4.JPG\")\n",
    "\n",
    "#image = mpimg.imread(\"my_test_images/finding_hue.JPG\")\n",
    "#image = mpimg.imread(\"my_test_images/finding_hue2.JPG\")         #ismein yellow bohot kam ho gaya ab\n",
    "\n",
    "\n",
    "thresh_img, warped_img=process_image(image)\n",
    "\n",
    "def draw_subplot(img1,name1,img2,name2, img3,name3):\n",
    "    f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24, 9))\n",
    "    f.tight_layout()\n",
    "    \n",
    "    ax1.imshow(img1)  #Needs an RGB image for 3D images. For 2D images, it auto-colors them so use cmap='gray' to get grayscale if needed\n",
    "    ax1.set_title(name1, fontsize=50)\n",
    "    ax2.imshow(img2)  #Needs an RGB image for 3D images. For 2D images, it auto-colors them so use cmap='gray' to get grayscale if needed\n",
    "    ax2.set_title(name2, fontsize=50)\n",
    "    ax3.imshow(img3)\n",
    "    ax3.set_title(name3, fontsize=50)\n",
    "    plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\n",
    "    \n",
    "#draw_subplot(image,\"OG\",output,\"lala lala image\")\n",
    "draw_subplot(image, \"OG image\",thresh_img,\"Thresh_img\",warped_img,\"Bird eye's view\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to atleast stack binary images to form color images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset\n",
    "If your sanity checks reveal that the lane lines you've detected are problematic for some reason, you can simply assume it was a bad or difficult frame of video, retain the previous positions from the frame prior and step to the next frame to search again. If you lose the lines for several frames in a row, you should probably start searching from scratch using a histogram and sliding window, or another method, to re-establish your measurement.\n",
    "\n",
    "## Smoothing\n",
    "Even when everything is working, your line detections will jump around from frame to frame a bit and it can be preferable to smooth over the last n frames of video to obtain a cleaner result. Each time you get a new high-confidence measurement, you can append it to the list of recent measurements and then take an average over n past measurements to obtain the lane position you want to draw onto the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_output = 'output_files/video_clips/project_video.mp4'\n",
    "## To speed up the testing process you may want to try your pipeline on a shorter subclip of the video\n",
    "## To do so add .subclip(start_second,end_second) to the end of the line below\n",
    "## Where start_second and end_second are integer values representing the start and end of the subclip\n",
    "## You may also uncomment the following line for a subclip of the first 5 seconds\n",
    "\n",
    "clip1 = VideoFileClip(\"project_video.mp4\")\n",
    "#clip1 = VideoFileClip(\"project_video.mp4\").subclip(0,1)\n",
    "\n",
    "project_clip = clip1.fl_image(process_image) #NOTE: this function expects color images! \n",
    "%time project_clip.write_videofile(project_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(project_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# challenge video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge_output = 'output_files/video_clips/challenge_video_old.mp4'\n",
    "## To speed up the testing process you may want to try your pipeline on a shorter subclip of the video\n",
    "## To do so add .subclip(start_second,end_second) to the end of the line below\n",
    "## Where start_second and end_second are integer values representing the start and end of the subclip\n",
    "## You may also uncomment the following line for a subclip of the first 5 seconds\n",
    "\n",
    "clip2 = VideoFileClip(\"challenge_video.mp4\")\n",
    "#clip2 = VideoFileClip(\"challenge_video.mp4\").subclip(0,1)\n",
    "\n",
    "challenge_clip = clip2.fl_image(process_image) #NOTE: this function expects color images! \n",
    "%time challenge_clip.write_videofile(challenge_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(challenge_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
